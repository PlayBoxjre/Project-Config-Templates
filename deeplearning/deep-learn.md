# 深度学习阅读笔记

| 开始时间 | 2018/3/1                                 |
| ---- | ---------------------------------------- |
| 阅读者  | 孔翔                                       |
| 书籍地址 | https://exacity.github.io/deeplearningbook-chinese/ |
|      |                                          |

## 目录

- 引言



---

## 1. 引言

本书讨论一种解决方案。 `该方案可以让计算机从经验中学习，并根据层次化的概念体系来理解世界，而每个概念则通过与某些相对简单的概念之间的关系来定义`。 让计算机从经验获取知识，可以避免由人类来给计算机形式化地指定它需要的所有知识。 ***层次化的概念让计算机构建较简单的概念来学习复杂概念***。 如果绘制出这些概念如何建立在彼此之上的图，我们将得到一张”深”（层次很多）的图。 基于这个原因，我们称这种方法为~**AI深度学习**

一些人工智能项目力求将关于世界的知识用形式化的语言进行硬编码(hard-code)。 计算机可以使用逻辑推理规则来自动地理解这些形式化语言中的声明。 这就是众所周知的**人工智能的知识库方法**。

依靠硬编码的知识体系面对的困难表明，AI~系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。 这种能力被称为**机器学习。**



许多人工智能任务都可以通过以下方式解决：先提取一个合适的特征集，然后将这些特征提供给简单的机器学习算法。 例如，对于通过声音鉴别说话者的任务来说，一个有用的特征是对其声道大小的估计。 这个特征为判断说话者是男性、女性还是儿童提供了有力线索。

然而，对于许多任务来说，我们很难知道应该提取哪些特征。

解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。 这种方法我们称之为表示学习。 学习到的表示往往比手动设计的表示表现得更好。 并且它们只需最少的人工干预，就能让AI系统迅速适应新的任务。 表示学习算法只需几分钟就可以为简单的任务发现一个很好的特征集，对于复杂任务则需要几小时到几个月。 手动为一个复杂的任务设计特征需要耗费大量的人工时间和精力；甚至需要花费整个社群研究人员几十年的时间。

**表示学习算法的典型例子是自编码器**

>  自编码器由一个编码器函数和一个解码器函数组合而成。 编码器函数将输入数据转换为一种不同的表示，而解码器函数则将这个新的表示转换到原来的形式。 我们期望当输入数据经过编码器和解码器之后尽可能多地保留信息，同时希望新的表示有各种好的特性， 这也是自编码器的训练目标。 为了实现不同的特性，我们可以设计不同形式的自编码器。

当设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的变差因素。

 在此背景下，”因素”这个词仅指代影响的不同来源；因素通常不是乘性组合。 这些因素通常是不能被直接观察到的量。 相反，它们可能是现实世界中观察不到的物体或者不可观测的力，但会影响可观测的量。 为了对观察到的数据提供有用的简化解释或推断其原因，它们还可能以概念的形式存在于人类的思维中。 它们可以被看作数据的概念或者抽象，帮助我们了解这些数据的丰富多样性。

>  当分析语音记录时，变差因素包括说话者的年龄、性别、他们的口音和他们正在说的词语。 当分析汽车的图像时，变差因素包括汽车的位置、它的颜色、太阳的角度和亮度。

显然，从原始数据中提取如此高层次、抽象的特征是非常困难的。 许多诸如说话口音这样的变差因素，只能通过对数据进行复杂的、接近人类水平的理解来辨识。 这几乎与获得原问题的表示一样困难，因此，乍一看，表示学习似乎并不能帮助我们。

**深度学习通过其他较简单的表示来表达复杂表示，解决了表示学习中的核心问题。**

深度学习让计算机通过较简单概念构建复杂的概念。

 深度学习模型的典型例子是前馈深度网络或多层感知机

**多层感知机**仅仅是一个将一组输入值映射到输出值的数学函数。 该函数由许多较简单的函数复合而成。 我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。

***学习数据的正确表示的想法是解释深度学习的一个视角。 另一个视角是深度促使计算机学习一个多步骤的计算机程序。*** 

> 每一层表示都可以被认为是并行执行另一组指令之后计算机的存储器状态。 更深的网络可以按顺序执行更多的指令。 顺序指令提供了极大的能力，因为后面的指令可以参考早期指令的结果。 从这个角度上看，在某层激活函数里，并非所有信息都蕴涵着解释输入的变差因素。 表示还存储着状态信息，用于帮助程序理解输入。 这里的状态信息类似于传统计算机程序中的计数器或指针。 它与具体的输入内容无关，但有助于模型组织其处理过程。

**目前主要有两种度量模型深度的方式**

- 第一种方式是基于评估架构所需执行的顺序指令的数目。

  > 假设我们将模型表示为给定输入后，计算对应输出的流程图，则可以将这张流程图中的最长路径视为模型的深度。

- 另一种是在深度概率模型中使用的方法

  > 它不是将计算图的深度视为模型深度，而是将描述概念彼此如何关联的图的深度视为模型深度
  >
  >  在这种情况下，计算每个概念表示的计算流程图的深度可能比概念本身的图更深。 这是因为系统对较简单概念的理解在给出更复杂概念的信息后可以进一步精细化。 例如，一个~AI~系统观察其中一只眼睛在阴影中的脸部图像时，它最初可能只看到一只眼睛。 但当检测到脸部的存在后，系统可以推断第二只眼睛也可能是存在的。 在这种情况下，概念的图仅包括两层（关于眼睛的层和关于脸的层），但如果我们细化每个概念的估计将需要额外的$n$次计算，即计算的图将包含$2n$层。

总之， 这本书的主题——深度学习是通向人工智能的途径之一。 具体来说，它是机器学习的一种，一种能够使计算机系统从经验和数据中得到提高的技术。 我们坚信机器学习可以构建出在复杂实际环境下运行的~AI~系统，并且是唯一切实可行的方法。 深度学习是一种特定类型的机器学习，具有强大的能力和灵活性，它将大千世界表示为嵌套的层次概念体系 （由较简单概念间的联系定义复杂概念、从一般抽象概括到高级抽象表示）



我们假设所有读者都具备计算机科学背景。 也假设读者熟悉编程，并且对计算的性能问题、复杂性理论、入门级微积分和一些图论术语有基本的了解。

 分布式表示的概念是本书的核心，我们将在\chap?中更加详细地描述。

**联结主义的中心思想是**，当网络将大量简单的计算单元连接在一起时可以实现智能行为。 这种见解同样适用于生物神经系统中的神经元，因为它和计算模型中隐藏单元起着类似的作用。

- 其中一个概念是分布式表示{cite?}。 其思想是：系统的每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能输入的表示。

  > 例如，假设我们有一个能够识别红色、绿色、或蓝色的汽车、卡车和鸟类的视觉系统， 表示这些输入的其中一个方法是将九个可能的组合：红卡车，红汽车，红鸟，绿卡车等等使用单独的神经元或隐藏单元激活。 这需要九个不同的神经元，并且每个神经必须独立地学习颜色和对象身份的概念。 改善这种情况的方法之一是使用分布式表示，即用三个神经元描述颜色，三个神经元描述对象身份。 这仅仅需要6个神经元而不是9个，并且描述红色的神经元能够从汽车、卡车和鸟类的图像中学习红色，而不仅仅是从一个特定类别的图像中学习。

  分布式表示的概念是本书的核心

- 重要成就是反向传播在训练具有内部表示的深度神经网络中的成功使用以及反向传播算法的普及{cite?}。 这个算法虽然曾黯然失色不再流行，但截至写书之时，它仍是训练深度模型的主导方法。

在20世纪90年代，研究人员在使用**神经网络进行序列建模**的方面取得了重要进展。 {Hochreiter91}和~{Bengio-trnn93-small}指出了对长序列进行建模的一些根本性数学难题，这将在\sec?中描述。 {Hochreiter+Schmidhuber-1997}**引入长短期记忆网络**来解决这些难题。 如今，LSTM~在许多序列建模任务中广泛应用，包括Google的许多自然语言处理任务。



同时，机器学习的其他领域取得了进步。 比如，核方法{cite?}和图模型{cite?}都在很多重要任务上实现了很好的效果。 这两个因素导致了神经网络热潮的第二次衰退，并一直持续到2007年。



Geoffrey Hinton~表明名为深度信念网络的神经网络可以使用一种称为**贪婪逐层预训练**的策略来有效地训练

第三次浪潮已开始着眼于新的无监督学习技术和深度模型在小数据集的泛化能力，但目前更多的兴趣点仍是比较传统的监督学习算法和深度模型充分利用大型标注数据集的能力

监督深度学习算法在每类给定约5000个标注样本情况下一般将达到可以接受的性能，当至少有1000万个标注样本的数据集用于训练时，它将达到或超过人类表现。 此外，在更小的数据集上获得成功是一个重要的研究领域，为此我们应特别侧重于如何通过无监督或半监督学习充分利用大量的未标注样本

 生物神经网络规模来自{number_of_neurons}。 } {\tiny \begin{enumerate} +sep0em

- % 1 感知机~{cite?}
- % 2 自适应线性单元~{cite?}
- % 3 神经认知机~{cite?}
- % 4 早期后向传播网络~{cite?}
- % 5 用于语音识别的循环神经网络~{cite?}
- % 6 用于语音识别的多层感知机~{cite?}
- % 7 均匀场sigmoid信念网络~{cite?}
- % 8 LeNet-5~{cite?}
- % 9 回声状态网络~{cite?}
- % 10 深度信念网络~{cite?}
- % 11 GPU-加速卷积网络~{cite?}
- % 12 深度玻尔兹曼机~{cite?}
- % 13 GPU-加速深度信念网络~{cite?}
- % 14 无监督卷积网络~{cite?}
- % 15 GPU-加速多层感知机~{cite?}
- % 16 OMP-1 网络~{cite?}
- % 17 分布式自编码器~{cite?}
- % 18 Multi-GPU卷积网络~{cite?}
- % 19 COTS HPC 无监督卷积网络~{cite?}